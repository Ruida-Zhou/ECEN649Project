{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "class SVM:\n",
    "    def __init__(self, data_len, lbd):\n",
    "        self.W = np.zeros(data_len)\n",
    "        self.b = 0\n",
    "        self.lbd = lbd\n",
    "    \n",
    "    def train(self, X, Y, eta, epsilon, batch_size):\n",
    "        batch_size = min(batch_size, len(Y))\n",
    "        \n",
    "        cnt = 0\n",
    "        sqr_sum_W = np.ones(len(self.W))\n",
    "        sqr_sum_b = 1\n",
    "        e = 1\n",
    "        \n",
    "        stop = 0\n",
    "        while stop<100:\n",
    "            batch = random.sample(range(len(Y)), batch_size)\n",
    "            X_batch, Y_batch = X[batch], Y[batch]\n",
    "            dev_W, dev_b = self.derivative(X_batch, Y_batch)\n",
    "            sqr_sum_W = 0.9*sqr_sum_W + 0.1*dev_W*dev_W\n",
    "            sqr_sum_b = 0.9*sqr_sum_b + 0.1*dev_b*dev_b\n",
    "            \n",
    "            self.W = self.W - eta*dev_W/np.sqrt(sqr_sum_W + e*np.ones(len(self.W)))\n",
    "            self.b = self.b - eta*dev_b/np.sqrt(sqr_sum_b + e)\n",
    "            if np.dot(dev_W, dev_W) + np.dot(dev_b, dev_b) < epsilon:\n",
    "                break\n",
    "            if cnt==20:\n",
    "                print(np.dot(dev_W, dev_W) + np.dot(dev_b, dev_b), self.loss(X,Y))\n",
    "                cnt = 0\n",
    "                stop = stop + 1\n",
    "            cnt = cnt + 1\n",
    "        return\n",
    "    \n",
    "    def derivative(self, X, Y):\n",
    "        dev_W = np.zeros(len(self.W))\n",
    "        dev_b = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            y = 2*(y-1/2)\n",
    "            if 1 > y*(np.dot(self.W, x) + self.b):\n",
    "                dev_W = dev_W - y*x\n",
    "                dev_b = dev_b - y\n",
    "        dev_W = dev_W / len(Y)\n",
    "        dev_b = dev_b / len(Y)\n",
    "        dev_W = dev_W + self.lbd * 2 * self.W\n",
    "        #print(dev_W, dev_b)\n",
    "        return dev_W, dev_b\n",
    "        \n",
    "    def loss(self, X, Y):\n",
    "        l = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            y = 2*(y-1/2)\n",
    "            l += max(0, 1 - y*(np.dot(self.W, x) + self.b))\n",
    "        l /= len(Y)\n",
    "        l += self.lbd * np.dot(self.W, self.W)\n",
    "        return l\n",
    "    \n",
    "    def output(self, x):\n",
    "        if np.dot(self.W, x) + self.b > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        error = 0\n",
    "        for x, y in zip(X, Y):\n",
    "            y = 2*(y-1/2)\n",
    "            if y!=self.output(x):\n",
    "                error = error + 1\n",
    "        return error/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_data(s, delete_name):\n",
    "    '''\n",
    "    s, <string>, which dataset to access\n",
    "    delete_name, <set>, which features do not take into account\n",
    "    '''\n",
    "    if s=='test':\n",
    "        df = pd.read_csv('pro_data_test.csv', delimiter=',')\n",
    "    elif s=='train':\n",
    "        df = pd.read_csv('pro_data_train.csv', delimiter=',')\n",
    "    else:\n",
    "        return\n",
    "    columns = df.columns\n",
    "    for column in columns:\n",
    "        if column in delete_name:\n",
    "            del df[column]\n",
    "    #print(df.head())\n",
    "    feature_len = len(df.columns)-1\n",
    "    feature = df[df.columns[0:feature_len-1]]\n",
    "    label = df[df.columns[feature_len:feature_len+1]]\n",
    "    X = feature.values\n",
    "    Y = label.values\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_name = {'fnlwgt', 'capital-gain', 'capital-loss'}\n",
    "X_train, Y_train = read_data('train', delete_name)\n",
    "X_test, Y_test = read_data('test', delete_name)\n",
    "#print(len(X_train[0]), len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbd = 1\n",
    "svm = SVM(len(X_train[0]), lbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664.3567229743333 [0.8639851]\n",
      "1012.5969858959824 [0.76742507]\n",
      "1125.1538542903168 [0.67229343]\n",
      "179.17435123221972 [0.60549806]\n",
      "13.531071935715039 [0.58969033]\n",
      "10.60511184170198 [0.5861775]\n",
      "8.96576321082867 [0.58340679]\n",
      "1.8211177509244754 [0.58101817]\n",
      "23.11923187063066 [0.57911613]\n",
      "15.923972550519785 [0.57728062]\n",
      "0.7568453037182211 [0.57576936]\n",
      "3.267687345700525 [0.57447846]\n",
      "2.40647533359558 [0.57336351]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "eta = 0.0001\n",
    "batch_size = 200\n",
    "svm.train(X_train, Y_train, eta, epsilon, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2456839309428951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.test(X_test,Y_test)\n",
    "#print(len(svm.W), len(X_test[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
